# -*- coding: utf-8 -*-
"""Mental Health Support Chatbot (Fine-Tuned)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pJMA7AUUgpS4KrSTmEqIXU0OAW8w-1Bb
"""

pip install transformers datasets torch sentencepiece accelerate streamlit

from datasets import load_dataset
import pandas as pd

# 1. Load Dataset (choose ONE method based on your file type)
# ----------------------------------------------------------

# Method A: If you have a CSV file (replace with your actual path)
dataset = load_dataset('csv', data_files='/content/emotion-emotion_69k.csv')

# Method B: If you have a JSON file
# dataset = load_dataset('json', data_files='path/to/your/empathetic_dialogues.json')

# 2. Verify the dataset structure
print("Dataset structure:", dataset)
print("\nFirst example:", dataset['train'][0])  # Check column names

# 3. Preprocess (rename columns if needed)
# ----------------------------------------------------------
# EmpatheticDialogues typically has columns: 'context', 'utterance', 'speaker_idx'
# If your columns are different, rename them to match expected format
def preprocess(example):
    return {
        "text": f"Context: {example['context']}\nUser: {example['utterance']}\nBot:"  # Format for fine-tuning
    }

dataset = dataset.map(preprocess)

# 4. Verify preprocessing
print("\nAfter preprocessing:", dataset['train'][0]['text'])

from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # or "EleutherAI/gpt-neo-125M"
model = AutoModelForCausalLM.from_pretrained("distilgpt2")

tokenizer.pad_token = tokenizer.eos_token

from datasets import load_dataset

# Load dataset from your file
dataset = load_dataset('csv', data_files='/content/emotion-emotion_69k.csv')  # or 'json' if JSON file

# Now tokenize (replace "text" with actual column name containing dialogues)
def tokenize_function(examples):
    return tokenizer(examples["utterance"], truncation=True, padding="max_length", max_length=128)  # Use "utterance" or the correct column name

tokenized_dataset = dataset.map(tokenize_function, batched=True)